# -*- coding: utf-8 -*-
"""tabla_seguridad_social_.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19_guggmkGpdvOhTvIH1nRYymlKGTcrCb
"""

# @title
# ==== INSTALAR DEPENDENCIAS ====
!apt-get update
!apt-get install -y tesseract-ocr
!apt-get install -y poppler-utils
!pip install pytesseract pdf2image pymupdf pandas opencv-python pdfplumber google-generativeai ipywidgets --quiet


# ==== SUBIR PDFs DESDE TU COMPUTADOR ====
from google.colab import files
import os
import re
import fitz  # PyMuPDF
import pytesseract
from pdf2image import convert_from_path
import cv2
import numpy as np
import pandas as pd
import pdfplumber
import json
import google.generativeai as genai
import ipywidgets as widgets
from IPython.display import display, FileLink, clear_output, HTML
import shutil


# OCR espa√±ol
pytesseract.pytesseract.tesseract_cmd = "/usr/bin/tesseract"

# ==== CONFIGURACI√ìN DE LA IA ====
GOOGLE_API_KEY = input('Ingresa tu API Key de Google Gemini (v√°lida, obtenla en https://aistudio.google.com/app/apikey): ')
genai.configure(api_key=GOOGLE_API_KEY)
model = genai.GenerativeModel('gemini-pro-latest') # Usamos 'gemini-pro-latest' o un modelo adecuado
print("Modelo de IA 'gemini-pro-latest' inicializado.")

# ==== FUNCIONES DE EXTRACCI√ìN ====
def ocr_pdf(path):
    """Convierte PDF escaneado en texto usando OCR."""
    texto = ""
    try:
        paginas = convert_from_path(path, dpi=300)
        for pagina in paginas:
            img = cv2.cvtColor(np.array(pagina), cv2.COLOR_BGR2RGB)
            texto += pytesseract.image_to_string(img, lang="spa") + "\n"
    except Exception as e:
        print(f"Advertencia: Error al realizar OCR en {path}: {e}")
        return ""
    return texto

def extraer_texto(path):
    """Intenta leer PDF digital, luego pdfplumber para texto/tablas, y si no, hace OCR. Returns (text, tables_list)."""
    contenido = ""
    tables_list = []
    doc = None
    try:
        doc = fitz.open(path)
        for pagina in doc:
            contenido += pagina.get_text()
    except ValueError as e:
        print(f"Advertencia: Error al abrir o leer PDF {path} con PyMuPDF: {e}. Intentando con pdfplumber.")
        contenido = "" # Reset content to try pdfplumber
    except Exception as e:
        print(f"Advertencia: Error inesperado al leer PDF {path} con PyMuPDF: {e}. Intentando con pdfplumber.")
        contenido = "" # Reset content to try pdfplumber
    finally:
        if doc: # If doc was successfully opened, ensure it's closed
            doc.close()

    # If fitz didn't get enough content, try pdfplumber
    if len(contenido.strip()) < 50:
        try:
            with pdfplumber.open(path) as pdf:
                pdfplumber_text = ""
                for page in pdf.pages:
                    page_text = page.extract_text(x_tolerance=1, y_tolerance=1)
                    if page_text:
                        pdfplumber_text += page_text + "\n"

                    tables = page.extract_tables()
                    for table in tables:
                        tables_list.append(table) # Append each table as a list of lists
                        # Optionally add table content to general text for general regex fallback
                        for row in table:
                            pdfplumber_text += " ".join(filter(None, [str(cell) if cell is not None else '' for cell in row])) + "\n"

                if len(pdfplumber_text.strip()) > 50: # If pdfplumber found content
                    return pdfplumber_text, tables_list
                else:
                    print(f"Advertencia: Texto digital escaso con pdfplumber en {path}. Intentando OCR.")
                    return ocr_pdf(path), []
        except Exception as e:
            print(f"Advertencia: Error al procesar PDF {path} con pdfplumber: {e}. Intentando OCR.")
            return ocr_pdf(path), []

    # If fitz already got sufficient content, return it with an empty tables list
    return contenido, []

# Refined REGEX patterns based on analysis
regex_cedula = r"(?:CC|C\.C|Identificaci[o√≥]n|NIT)[:\s]*(\d{6,10})\b"

regex_nombre_labeled = r"(?:Nombre|Cotizante|Afiliado|Razon Social|Cliente|EMPLEADO|APORTANTE|Nombres)[:\s]*([A-Z√Å√â√ç√ì√ö√ë][A-Z√Å√â√ç√ì√ö√ëa-z√°√©√≠√≥√∫√± ]*(?:[ \t][A-Z√Å√â√ç√ì√ö√ëa-z√°√©√≠√≥√∫√± ]+)*)(?=\s*\n|IBC|VALOR|Periodo|Total|Identificaci[o√≥]n|\d{6,10}|MES|SALUD|PENSION|RIESGOS|APORTE|$)"

regex_mes = r"(Enero|Febrero|Marzo|Abril|Mayo|Junio|Julio|Agosto|Septiembre|Octubre|Noviembre|Diciembre)(?: de \d{4})?|(?:Periodo.*?(?P<year_month>\d{4}-\d{2}))"

numeric_pattern = r"\d{1,3}(?:\.\d{3})*(?:,\d{1,2})?"

regex_ibc = r"(?:IBC|Base de Cotizaci[o√≥]n|Salud\s+IBC|PENSION\s+IBC)[:\s]*(?:\$?\s*)?(" + numeric_pattern + r")"

regex_salud = r"(?:Salud|Salud\s+Aporte|Salud\s+IBC)[:\s]*(?:\$?\s*)?(" + numeric_pattern + r")"

regex_valor_sin_mora_labeled = r"(?:Valor sin Mora|VALOR LIQUIDADO)[:\s]*(?:\$?\s*)?(" + numeric_pattern + r")"
regex_aporte_valor = r"(?:Aporte|Valor\s*(?![a-zA-Z]*Mora|total a pagar|de la|pagado|pagar))[:\s]*(?:\$?\s*)?(" + numeric_pattern + r")"

regex_mora = r"(?:Mora|Intereses Mora|INTERESES MORA|Total Int[e√©]reses Mora)[:\s]*(?:\$?\s*)?(" + numeric_pattern + r")"

def limpiar(num):
    if not num:
        return ""
    return num.replace(".", "").replace(",", ".").replace("$", "").strip()

def limpiar_ia_valor(value):
    """Limpia y estandariza los valores num√©ricos extra√≠dos por la IA."""
    if not value:
        return ""
    cleaned_value = str(value).replace(".", "").replace(",", ".").replace("$", "").strip()
    if re.fullmatch(r"\d+(\.\d+)?", cleaned_value):
        return cleaned_value
    return ""

def extraer_campos_con_ia(texto_pdf):
    """Utiliza un LLM para extraer campos clave de un texto de PDF."""
    prompt = f"""Dada el siguiente texto de un documento PDF, extrae la C√©dula (entre 6 y 10 d√≠gitos), el Nombre completo de la persona/empresa (puede estar etiquetado como Nombre, Cotizante, Afiliado, Razon Social, Cliente, Empleado o Aportante), el Mes de cotizaci√≥n (solo el nombre del mes, ej. 'Octubre'), el IBC (Ingreso Base de Cotizaci√≥n), el valor de Salud, el Valor Sin Mora y el Valor Con Mora. Si un campo no se encuentra, deja su valor como una cadena vac√≠a. Los valores num√©ricos pueden usar punto como separador de miles y coma como separador decimal. Devuelve la informaci√≥n como un objeto JSON con las siguientes claves: 'C√©dula', 'Nombre', 'Mes', 'IBC', 'Salud', 'Valor Sin Mora', 'Valor Con Mora'.\n\nTexto:\n\n{texto_pdf}"""

    cedula = ""
    nombre = ""
    mes = ""
    ibc = ""
    salud = ""
    valor_sin_mora = ""
    mora = ""
    valor_con_mora = ""

    try:
        response = model.generate_content(prompt)
        if response.text:
            json_text = response.text.strip().replace('```json', '').replace('```', '')
            extracted_data = json.loads(json_text)

            cedula = extracted_data.get('C√©dula', '')
            nombre = extracted_data.get('Nombre', '')
            mes = extracted_data.get('Mes', '')
            ibc = limpiar_ia_valor(extracted_data.get('IBC', ''))
            salud = limpiar_ia_valor(extracted_data.get('Salud', ''))
            valor_sin_mora = limpiar_ia_valor(extracted_data.get('Valor Sin Mora', ''))
            mora = limpiar_ia_valor(extracted_data.get('Mora', ''))
            llm_valor_con_mora = extracted_data.get('Valor Con Mora', '')
            if llm_valor_con_mora:
                valor_con_mora = limpiar_ia_valor(llm_valor_con_mora)

    except json.JSONDecodeError as e:
        print(f"Advertencia: Error al decodificar JSON de la respuesta del LLM: {e}")
        print(f"Texto de respuesta del LLM: {response.text}")
    except Exception as e:
        print(f"Advertencia: Error al generar contenido con LLM: {e}")

    return cedula, nombre, mes, ibc, salud, valor_sin_mora, mora, valor_con_mora

def extraer_campos(texto, tablas_extraidas=None):
    if tablas_extraidas is None:
        tablas_extraidas = []

    # --- Step 1 & 2: Try AI extraction first ---
    cedula_ia, nombre_ia, mes_ia, ibc_ia, salud_ia, valor_sin_mora_ia, mora_ia, valor_con_mora_ia = extraer_campos_con_ia(texto)

    # --- Step 3: Initialize fields with AI results, fallback if empty ---
    cedula = cedula_ia
    nombre = nombre_ia
    mes = mes_ia
    ibc = ibc_ia
    salud = salud_ia
    valor_sin_mora = valor_sin_mora_ia
    mora = mora_ia
    valor_con_mora = valor_con_mora_ia

    # If C√©dula is not found by IA, try with regex
    if not cedula:
        c = re.search(regex_cedula, texto, re.IGNORECASE)
        if c:
            cedula = c.group(1).strip()

    # If Nombre is not found by IA, try with regex and heuristics
    if not nombre:
        n_labeled = re.search(regex_nombre_labeled, texto, re.IGNORECASE | re.DOTALL)
        if n_labeled:
            nombre = n_labeled.group(1).strip().title()

        if not nombre and cedula:
            n_after_cc_pattern = re.search(
                r"(?:CC|C\.C|Identificaci[o√≥]n)\s*" + re.escape(cedula) + r"\s*\n?\s*([A-Z√Å√â√ç√ì√ö√ë][A-Z√Å√â√ç√ì√ö√ëa-z√°√©√≠√≥√∫√± ]*(?:[ \t][A-Z√Å√â√ç√ì√ö√ëa-z√°√©√≠√≥√∫√± ]+)*)(?=\s*\n|IBC|VALOR|Periodo|Total|Identificaci[o√≥]n|\d{6,10}|MES|SALUD|PENSION|RIESGOS|APORTE|$)",
                texto,
                re.IGNORECASE | re.DOTALL
            )
            if n_after_cc_pattern:
                potential_name = n_after_cc_pattern.group(1).strip()
                if (len(potential_name.split()) > 1 and
                    all(word[0].isupper() or len(word) <= 2 for word in potential_name.split()) and
                    not any(char.isdigit() for char in potential_name)):
                    if not any(kw in potential_name.upper() for kw in [
                        "INFORMACI\u00d3N", "PLANILLA", "PAGADA", "TOTAL", "APORTANTE", "PERIODO",
                        "SUCURSAL", "EMPLEADO", "PENSION", "SALUD", "ARL", "CCF", "CODIGO",
                        "VALOR", "RESUMEN", "GENERAL", "PAGO", "FECHA", "EMPRESA", "BANCO",
                        "DESCRIPCION", "NUMERO", "IDENTIFICACION", "NIT", "DIAS", "BASE", "COTIZACION",
                        "REGIMEN", "CAJA", "COMPENSACION", "FONDOS", "SOLIDARIDAD", "LABORAL", "VACACIONES",
                        "MES", "CIUDAD", "SENA", "ADMINISTRADORA"]
                    ):
                        nombre = potential_name.title()
            if not nombre:
                 n_same_line_after_cc_pattern = re.search(
                    r"(?:CC|C\.C|Identificaci[o√≥]n)\s*" + re.escape(cedula) + r"\s*([A-Z√Å√â√ç√ì√ö√ë][A-Z√Å√â√ç√ì√ö√ëa-z√°√©√≠√≥√∫√± ]*(?:[ \t][A-Z√Å√â√ç√ì√ö√ëa-z√°√©√≠√≥√∫√± ]+)*)",
                    texto,
                    re.IGNORECASE
                )
                 if n_same_line_after_cc_pattern:
                     potential_name = n_same_line_after_cc_pattern.group(1).strip()
                     if (len(potential_name.split()) > 1 and
                         all(word[0].isupper() or len(word) <= 2 for word in potential_name.split()) and
                         not any(char.isdigit() for char in potential_name)):
                         if not any(kw in potential_name.upper() for kw in [
                             "INFORMACI\u00d3N", "PLANILLA", "PAGADA", "TOTAL", "APORTANTE", "PERIODO",
                             "SUCURSAL", "EMPLEADO", "PENSION", "SALUD", "ARL", "CCF", "CODIGO",
                             "VALOR", "RESUMEN", "GENERAL", "PAGO", "FECHA", "EMPRESA", "BANCO",
                             "DESCRIPCION", "NUMERO", "IDENTIFICACION", "NIT", "DIAS", "BASE", "COTIZACION",
                             "REGIMEN", "CAJA", "COMPENSACION", "FONDOS", "SOLIDARIDAD", "LABORAL", "VACACIONES",
                             "MES", "CIUDAD", "SENA", "ADMINISTRADORA"]
                         ):
                             nombre = potential_name.title()


    # If Mes is not found by IA, try with regex
    if not mes:
        m = re.search(regex_mes, texto, re.IGNORECASE)
        if m:
            if m.group(1): # if month name found directly (e.g., 'Octubre')
                mes = m.group(1).capitalize() # Ensure consistent capitalization
            elif m.group('year_month'): # if YYYY-MM found
                year_month_str = m.group('year_month')
                month_num = int(year_month_str.split('-')[1])
                meses_map = {
                    1: "Enero", 2: "Febrero", 3: "Marzo", 4: "Abril", 5: "Mayo", 6: "Junio",
                    7: "Julio", 8: "Agosto", 9: "Septiembre", 10: "Octubre", 11: "Noviembre", 12: "Diciembre"
                }
                mes = meses_map.get(month_num, "")

    # --- Step 4: Fallback for numeric fields if IA didn't find them ---
    # Prioritize table extraction first if 'cedula' is available to locate the data row
    if (not ibc or not salud or not valor_sin_mora or not mora) and cedula:
        for table in tablas_extraidas:
            potential_col_maps = {} # To store multiple possible header interpretations
            id_col_idx = -1 # Primary ID column for the cotizante

            # First pass: try to find the ID column and map other header keywords.
            # We'll scan a few initial rows to be robust to multi-line headers.
            for r_idx, row_cells in enumerate(table):
                # Clean up cells for robust matching
                cleaned_row = [str(cell).strip().upper() if cell else '' for cell in row_cells]

                # Identify ID column (e.g., 'Identificacion', 'CC', 'Cedula')
                if id_col_idx == -1: # Only try to set ID column once for the table
                    for c_idx, cell_value in enumerate(cleaned_row):
                        if re.search(r"IDENTIFICACION|CEDULA|NO\.", cell_value, re.IGNORECASE): # Corrected: NO\. for literal dot
                            id_col_idx = c_idx
                            break

                # Build a comprehensive column map for the table based on all rows or top rows
                for c_idx, cell_value in enumerate(cleaned_row):
                    if re.search(r"IBC|BASE DE COTIZACION", cell_value, re.IGNORECASE):
                        potential_col_maps.setdefault('ibc_cols', []).append(c_idx)
                    elif re.search(r"SALUD|EPS APORTE", cell_value, re.IGNORECASE): # New: Salud column
                        potential_col_maps.setdefault('salud_cols', []).append(c_idx)
                    elif re.search(r"VALOR SIN MORA|VALOR LIQUIDADO|APORTE", cell_value, re.IGNORECASE) and not re.search(r"MORA", cell_value, re.IGNORECASE):
                        potential_col_maps.setdefault('valor_cols', []).append(c_idx)
                    elif re.search(r"MORA|INTERESES MORA", cell_value, re.IGNORECASE):
                        potential_col_maps.setdefault('mora_cols', []).append(c_idx)

            # Second pass: find the data row by cedula and extract values
            if id_col_idx != -1: # If we successfully identified an ID column
                for r_idx, row_cells in enumerate(table):
                    current_row_data = [str(cell) if cell is not None else '' for cell in row_cells]

                    # Check if cedula is in the identified ID column of this row
                    if id_col_idx < len(current_row_data) and cedula == limpiar(current_row_data[id_col_idx]):
                        # This is likely the data row for the cotizante

                        # IBC
                        if not ibc and 'ibc_cols' in potential_col_maps:
                            for col_idx in potential_col_maps['ibc_cols']:
                                if col_idx < len(current_row_data):
                                    potential_ibc = current_row_data[col_idx]
                                    if re.search(numeric_pattern, potential_ibc):
                                        ibc = limpiar(potential_ibc)
                                        break # Found IBC, move to next field

                        # Salud
                        if not salud and 'salud_cols' in potential_col_maps:
                            for col_idx in potential_col_maps['salud_cols']:
                                if col_idx < len(current_row_data):
                                    potential_salud = current_row_data[col_idx]
                                    if re.search(numeric_pattern, potential_salud):
                                        salud = limpiar(potential_salud)
                                        break # Found Salud, move to next field

                        # Valor Sin Mora
                        if not valor_sin_mora and 'valor_cols' in potential_col_maps:
                            for col_idx in potential_col_maps['valor_cols']:
                                if col_idx < len(current_row_data):
                                    potential_valor = current_row_data[col_idx]
                                    if re.search(numeric_pattern, potential_valor):
                                        valor_sin_mora = limpiar(potential_valor)
                                        break

                        # Mora
                        if not mora and 'mora_cols' in potential_col_maps:
                            for col_idx in potential_col_maps['mora_cols']:
                                if col_idx < len(current_row_data):
                                    potential_mora = current_row_data[col_idx]
                                    if re.search(numeric_pattern, potential_mora):
                                        mora = limpiar(potential_mora)
                                        break

                        # If all numeric fields are found, break from current table
                        if ibc and salud and valor_sin_mora and mora:
                            break
            if ibc and salud and valor_sin_mora and mora: # Break outer table loop too
                break


    # --- Fallback to text extraction if not found in tables (and not found by IA) --- START
    if not ibc:
        if cedula:
            cedula_start_index = texto.find(cedula)
            if cedula_start_index != -1:
                window_size = 1000 # characters before and after the cedula
                search_start = max(0, cedula_start_index - window_size)
                search_end = min(len(texto), cedula_start_index + window_size)
                context_text = texto[search_start:search_end]
                i = re.search(regex_ibc, context_text, re.IGNORECASE | re.DOTALL)
                if i:
                    ibc = limpiar(i.group(1))
        if not ibc: # Global search if still not found
            i = re.search(regex_ibc, texto, re.IGNORECASE | re.DOTALL)
            if i:
                ibc = limpiar(i.group(1))

    if not salud:
        if cedula:
            cedula_start_index = texto.find(cedula)
            if cedula_start_index != -1:
                window_size = 1000 # characters before and after the cedula
                search_start = max(0, cedula_start_index - window_size)
                search_end = min(len(texto), cedula_start_index + window_size)
                context_text = texto[search_start:search_end]
                s = re.search(regex_salud, context_text, re.IGNORECASE | re.DOTALL)
                if s:
                    salud = limpiar(s.group(1))
        if not salud: # Global search if still not found
            s = re.search(regex_salud, texto, re.IGNORECASE | re.DOTALL)
            if s:
                salud = limpiar(s.group(1))


    if not valor_sin_mora:
        if cedula:
            cedula_start_index = texto.find(cedula)
            if cedula_start_index != -1:
                window_size = 1000 # characters before and after the cedula
                search_start = max(0, cedula_start_index - window_size)
                search_end = min(len(texto), cedula_start_index + window_size)
                context_text = texto[search_start:search_end]
                v = re.search(regex_valor_sin_mora_labeled, context_text, re.IGNORECASE | re.DOTALL)
                if v:
                    valor_sin_mora = limpiar(v.group(1))
                elif not valor_sin_mora: # Try generic aporte/valor if specific labeled not found
                    v_gen = re.search(regex_aporte_valor, context_text, re.IGNORECASE | re.DOTALL)
                    if v_gen:
                        valor_sin_mora = limpiar(v_gen.group(1))
        if not valor_sin_mora: # Global search if still not found
            v = re.search(regex_valor_sin_mora_labeled, texto, re.IGNORECASE | re.DOTALL)
            if v:
                valor_sin_mora = limpiar(v.group(1))
            elif not valor_sin_mora: # Try generic aporte/valor if specific labeled not found
                v_gen = re.search(regex_aporte_valor, texto, re.IGNORECASE | re.DOTALL)
                if v_gen:
                    valor_sin_mora = limpiar(v_gen.group(1))

    if not mora:
        if cedula:
            cedula_start_index = texto.find(cedula)
            if cedula_start_index != -1:
                window_size = 1000 # characters before and after the cedula
                search_start = max(0, cedula_start_index - window_size)
                search_end = min(len(texto), cedula_start_index + window_size)
                context_text = texto[search_start:search_end]
                mo = re.search(regex_mora, context_text, re.IGNORECASE | re.DOTALL)
                if mo:
                    mora = limpiar(mo.group(1))
        if not mora: # Global search if still not found
            mo = re.search(regex_mora, texto, re.IGNORECASE | re.DOTALL)
            if mo:
                mora = limpiar(mo.group(1))
    # --- Fallback to text extraction if not found in tables --- END

    valor_con_mora = ""
    if valor_sin_mora and mora:
        try:
            val_sin_mora_float = float(valor_sin_mora)
            mora_float = float(mora)
            valor_con_mora = str(val_sin_mora_float + mora_float)
        except ValueError:
            valor_con_mora = ""

    return cedula, nombre, mes, ibc, salud, valor_sin_mora, valor_con_mora



# ==== PROCESAR PDFs Y GENERAR DATAFRAME ====
datos_extraidos = []
pdfs_dir = "pdfs"

# Ensure pdfs directory exists
os.makedirs(pdfs_dir, exist_ok=True)

def process_pdfs_and_display_df():
    global datos_extraidos, df
    datos_extraidos = [] # Clear previous data

    pdf_files = [f for f in os.listdir(pdfs_dir) if f.endswith(".pdf")]

    if not pdf_files:
        print("No hay PDFs en la carpeta 'pdfs' para procesar.")
        return

    for filename in pdf_files:
        pdf_path = os.path.join(pdfs_dir, filename)
        print(f"Procesando {filename}...")
        texto_pdf, tablas_pdf = extraer_texto(pdf_path)
        cedula, nombre, mes, ibc, salud, valor_sin_mora, valor_con_mora = extraer_campos(texto_pdf, tablas_pdf)

        datos_extraidos.append({
            "C√©dula": cedula,
            "Nombre": nombre,
            "Mes": mes,
            "IBC": ibc,
            "Salud": salud,
            "Valor Sin Mora": valor_sin_mora,
            "Valor Con Mora": valor_con_mora
        })

    print("Todos los PDFs han sido procesados y los datos extra√≠dos.")
    print(f"Total de registros extra√≠dos: {len(datos_extraidos)}")

    df = pd.DataFrame(datos_extraidos)
    print("DataFrame creado con los datos extra√≠dos:")
    display(df)

# ==== BOTONES FUNCIONALES ====
out = widgets.Output()

# BOT√ìN: SUBIR ARCHIVOS
bn_subir = widgets.Button(description="üì§ Subir PDFs", button_style='info')

def subir_archivos_handler(b):
    with out:
        clear_output(wait=True)
        print("Selecciona los PDFs...")
        uploaded = files.upload()

        os.makedirs("pdfs", exist_ok=True)
        for fname in uploaded.keys():
            with open("pdfs/" + fname, "wb") as f:
                f.write(uploaded[fname])

        print(f"Subidos {len(uploaded)} archivos a /pdfs")

bn_subir.on_click(subir_archivos_handler)

# BOT√ìN: BORRAR ARCHIVOS
bn_borrar = widgets.Button(description="üßπ Borrar PDFs", button_style='danger')

def borrar_archivos_handler(b):
    with out:
        clear_output(wait=True)
        if os.path.exists("pdfs"):
            shutil.rmtree("pdfs")
            os.makedirs("pdfs") # Recreate empty directory
            print("Carpeta '/pdfs' eliminada y recreada vac√≠a.")
        else:
            print("No existe la carpeta '/pdfs'.")

bn_borrar.on_click(borrar_archivos_handler)

# BOT√ìN: PROCESAR Y GENERAR TABLA
bn_procesar = widgets.Button(description="‚öô Procesar PDFs y Generar Tabla", button_style='success')

def procesar_archivos_handler(b):
    with out:
        clear_output(wait=True)
        print("Iniciando procesamiento de PDFs...")
        process_pdfs_and_display_df()
        output_csv_path = "datos_extraidos_pdfs.csv"
        df.to_csv(output_csv_path, index=False, encoding='utf-8')
        print(f"DataFrame guardado en: {output_csv_path}")
        display(FileLink(output_csv_path))
        print("Procesamiento completado y tabla actualizada.")

bn_procesar.on_click(procesar_archivos_handler)


# ==== INSTRUCCIONES INICIALES Y MOSTRAR INTERFAZ ====
display(HTML("<h3>Bienvenido a tu herramienta de extracci√≥n de datos de PDFs.</h3>"))
display(HTML("1. Haz clic en 'üì§ Subir PDFs' para cargar tus archivos (puedes subir varios a la vez).<br>"))
display(HTML("2. Luego, haz clic en '‚öô Procesar PDFs y Generar Tabla' para extraer los datos y ver la tabla.<br>"))
display(HTML("3. La tabla generada se guardar\u00e1 autom\u00e1ticamente como CSV, con un enlace para descargarla.<br>"))
display(HTML("4. Usa 'üßπ Borrar PDFs' si quieres eliminar los archivos cargados.<br><br>"))

widgets.VBox([
    widgets.HBox([bn_subir, bn_borrar, bn_procesar]),
    out
])